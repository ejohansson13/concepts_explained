Stable Diffusion was one of multiple text-to-image generation models to generate widespread public attention on the advancements of artificial intelligence and machine learning. Similar to the rise of the transformer architecture for natural language processing tasks, [Latent Diffusion Models (LDMs)](https://arxiv.org/pdf/2112.10752.pdf) quickly grew in popularity for conditional image generation. Previous image synthesis architectures included [GANs](https://arxiv.org/pdf/1605.05396.pdf), [RNNs](https://arxiv.org/pdf/1502.04623.pdf), and [autoregressive modeling](https://arxiv.org/pdf/1906.00446.pdf) which [multiple](https://arxiv.org/pdf/2102.12092.pdf) [companies](https://arxiv.org/pdf/2206.10789.pdf) favored. However, the performance of diffusion models in zero-shot applications as well as their efficiency operating in the latent space, led to LDMs becoming the eminent architecture for text-to-image generation tasks. Their compatibility with natural language, image, and audio prompts (thanks to cross-attention) accelerated the multimodality of machine learning applications. On this page, you'll find an overview of the Stable Diffusion architecture, its training procedure, and inference steps.

We'll be focusing on the text-to-image case, but I urge you to check out the [original paper](https://arxiv.org/pdf/2112.10752.pdf) which described multiple other use cases, including image inpainting, conditioning on semantic maps, and layout-to-image, with image generation conditioned on annotated bounding boxes.
<p align="center" width="100%">
  <img src="/Stable Diffusion/Images/SD_Images/LDM_diagram.png" alt="LDM diagram taken from original research paper" width="100%"
</p>

# Architecure

The latent diffusion network follows an encoder-decoder architecure. Between these blocks, there are three main stages: the scheduler, the denoising U-Net and the custom conditioner (text, image, or audio prompt for multimodal capabilities). The encoder is responsible for encoding images to their  latent representation. The U-Net is the vehicle carrying this latent representation through the latent space. The prompt suggests the latent space destination, and the scheduler guides the U-Net through the latent space to the preferred destination. This destination, decided by the U-Net, scheduler, and prompt is the closest latent space representation to the final image. The destination latent is then decoded and transformed to a pixel-space image adhering to the details described in the provided prompt.

Each of these stages will be covered in more detail below, and I've split the architecture details in two. [Training](#training) covers each stage's role throughout the training process. [Inference](#inference) offers the same details for the inference process, describing how user input is synthesized to a 512x512 pixel image.

## Training

Stable Diffusion models are trained on a wide variety of images pulled from the web. We pump in a large dataset of images for our network to learn a variety of visual themes and configurations. These images are taken apart through a forward process consisting of the time-controlled, sequential addition of Gaussian noise to a training image. This repetitive destruction of images teaches the network the probability distribution of destroying images but also the conditional probability of generating coherent images from random noise. The process of generating lucid images from noise is referred to as the reverse diffusion process. Importantly, LDMs were not the first network architecture to operate under the diffusion theory. The idea that models can learn to rebuild images from noise by decomposing training images can be found as early as [2015](https://arxiv.org/pdf/1503.03585.pdf). However, previous architectures operated on the pixel-space of images, reserving their utility for entities who could accomodate the resource-intensive training process. The distinction of latent diffusion models is autological. Rather than operate on the pixel-space, they first compact each image representation, encoding them to a latent space.
<p align="center" width="100%">
  <img src="/Stable Diffusion/Images/SD_Images/LDM_graph.png" alt="Graph demonstrating perceptual vs semantic compression training tradeoff taken from 2022 paper on latent diffusion models" width="50%">
</p>

The above graph demonstrates that the majority of bits constituting a digital image correspond to high-frequency, perceptual details. In contrast, relatively few bits comprise the semantic information of the image. Unlike previous diffusion models that tried to [jointly balance loss terms prioritizing the perceptual and semantic training](https://arxiv.org/pdf/2106.05931.pdf), latent diffusion models opt for a two-stage approach. Initially, the autoencoder compresses training images, creating a perceptually equivalent, but computationally cheaper latent space for semantic training. Learning the diffusion process is prioritized in the second stage of training. Semantic training freezes the autoencoder weights and prioritizes diffusion training, learning the reconstruction of coherent images from random white noise. 

### Autoencoder

Autoencoders have become ubiquitous across machine learning architecture. Their purpose is to receive an input and encode that input to a compressed representation with minimal information loss. The original LDM paper allowed for a pretrained autoencoder to accomplish this task. In practice, most implementations actually trained new autoencoders from scratch to adhere to the paper's loss functions. If you haven't already, I suggest you check out my page [explaining the U-Net](https://github.com/ejohansson13/concepts_explained/blob/main/UNet/UNet_ML.md). It gives a broad overview of an encoder-decoder architecture. Autoencoders for LDMs offer more complexity than the U-Net, but maintain the same principles.

Autoencoders are the first and last stage of Latent Diffusion Models. Their role is to create a stable, learnable latent space containing the distribution of compressed images from which future images can be generated. This space must contain all relevant semantic image information while obscuring superficial perceptual details. The latent space should be considered perceptually equivalent, while offering computationally friendly calculations in a lower-dimensional scope. Encoders perform this initial compression role, with decoders carrying the opposing responsibility. Decoders need to successfully rescale lower-dimensional latents to pixel-space images. Their weights need to be carefully calibrated to avoid the reintroduction of errors when scaling a latent to a higher-dimensional representation. Performing the myriad calculations in latent space only for the image's fidelity to be compromised by its transition to higher transitions would render latent-dimension modeling obsolete. 

Encoding to a latent space requires decisions on the size of the latent space. The authors experimented with multiple downsampling factors, ultimately determining that downsampling factors of 4 or 8 offered the best performance. Downsampling factors of 1 or 2 were considered prohibitively expensive, operating near pixel-space, and greatly slowing the training rate. Downsampling by a factor of 16 or more was determined to cause excessive information loss and low fidelity. Compression at that factor overtook the bits devoted to perceptual information and led to the cannibalization of semantic information present in the training data. We'll be focusing on downsampling by a factor of 8 which has proven more popular since the release of the original research paper. We'll refer to this specific model as LDM-8 for the remainder of the page.

#### Encoder
<p align="center" width="100%">
  <img src="/Stable Diffusion/Images/SD_Images/LDM_encoder_architecture.png" alt="Illustration of LDM encoder" width="100%"
</p>

Constituting the encoder are a variety of convolution operations, ResNet blocks, attention operators, activation functions, and downsampling. [One Stable Diffusion encoder implementation](https://github.com/CompVis/stable-diffusion/tree/main) is illustrated above. Through every step of the encoder operation, the aim is to efficiently condense image features. This is accomplished through an initial convolution, broadening the channels to 128, where pixel-space values are converted to feature embeddings. Following the initial convolution, a structure of two ResNet blocks preceding a downsampling operation appear. This structure is repeated until the feature embeddings are compressed to satisty the latent space dimensions. ResNet blocks function as the catalyst, activating the most important image features before downsampling funnels those features to lower-dimensional representations. Each downsampling operation halves both the height and width of the image to obtain a more compressed representation of the data. For LDM-4, this loop would occur twice, to quarter the image height and width. For LDM-8, this loop occurs three times to bring the latent dimensions to 1/8 of the original image dimensions. 

These ResNet blocks contain the following sequence of operations: group normalization, activation function, 3x3 convolution, group normalization, activation function, and another 3x3 convolution function before summing the original input. The repeated normalizations stabilize the data prior to a nonlinear activation function applied elementwise. 3x3 convolutions are the bread and butter of computer vision, emphasizing local feature dependencies while propagating data through the network. Residual paths between the input and output of the blocks ensure the preservation of information. Similar to skip connections in a U-Net, residual paths allow for the original feature information to be joined with the convolutional outputs and ensure relevant information from the original representation is propagated through the network. If the convolutional operations devalue image features relevant to the compression of data, residual paths give another opportunity for that data to advance in the network.

Outside of the ResNet->ResNet->downsampling loop, we find another grouping of ResNet blocks. These blocks follow the same constitution and perform equivalent functions to the blocks inside the loop. They serve to highlight relevant image features, propagate those features, and abstract superficial details superfluous to the semantic information of the image. With repeated normalizations, they also serve to stabilize the image data as it propagates through the network, preventing gradient instability from cascading through the network. This grouping of ResNet blocks has no further effect on the dimensionality of the images, they solely serve to detect, emphasize, stabilize, and propagate the most important features. 

Following the application of ResNet blocks, we arrive at an individual attention block, a departure from the ResNet and downsampling operations we have encountered so far. Convolution offers a number of benefits in detecting proximal image features. Holistic understanding of the image can require a different approach.  Similar to self-attention in text, applying self-attention to image features enables the understanding of longer interdependencies between features and broadens the network's understanding of the overall image. The comprehensive understanding of image features provided by self-attention is especially important at lower dimensions where the compression of perceptual information raises the risk of information loss. Applying self-attention in the encoder, at the same dimensions of the latent space, mitigates that risk. Following the application of self-attention, we apply another ResNet block before futher preparing our data for semantic training. Group normalization is followed by an activation function to model the nonlinear relationship of our encoder prior to a final convolutional operation. This final convolution determines the number of relevant channels within the latent space, preparing the data both for regularization and semantic training compatibility.

#### KL-regularization

To stabilize the latent space and prevent any pockets of high variance, we apply a low-penalty KL-regularization scheme. [KL-regularization has been shown to be very effective](https://proceedings.neurips.cc/paper/2020/file/8e2c381d4dd04f1c55093f22c59c3a08-Paper.pdf) in unifying diverging distributions. This serves to push the latent space to an approximation of a Gaussian distribution, smoothing out an otherwise unpredictable and high-variance encoding space. Unlike other encoder-decoder architectures, like the U-Net, the latent space for LDMs serves as more than an avenue to image reconstruction. The latent space of LDMs is a legitimate destination in its own right. All diffusion, generation, and denoising at inference time take place in this latent space; stabilizing this space affords a steadier venue for these operations. KL-regularization accomplishes this target, pushing the latent space distribution to an approximate Gaussian. Approximating a Gaussian distribution becomes important for two reasons: it simplifies the future sampling of random noise at inference time and removing Gaussian noise from an approximately Gaussian distribution allows for multiple steps to be skipped, accelerating the denoising process. 
Mention that the entire purpose of the regularizer is to foster a computationally-friendly latent space that is approximately Gaussian.

The original paper also included VQ-GAN regularization, but ultimately KL was determined to provide better results. Can include this information if desired. Include literature supporting this fact. 

#### Decoder
<p align="center" width="100%">
  <img src="/Stable Diffusion/Images/SD_Images/LDM_decoder_architecture.png" alt="Illustration of LDM decoder" width="100%"
</p>

Our decoder consists of the exact same blocks as our encoder in reverse order. Rather than downsampling, we'll now be upsampling. The fundamental goal of the decoder is to preserve the learned semantic information from our compressed image while scaling our image dimensions to pixel space. The latent space offers a perceptually equivalent compressed space and the decoder's role is to decompress our latent, and carefully calibrate the perceptual details of our image to the higher dimensions. We begin with an initial convolutional operation to translate our diffusion-scaled latent to the larger number of channels we'll need as we decode our image. We then encounter a ResNet->attention->ResNet trio of blocks. In both the encoder and decoder, self-attention operations are bookended by ResNet blocks. These serve to stabilize, convolve and conserve visual information immediately following the self-attention of image features. The immediate broadening of channels following the departure from latent space allows for more information to be considered by the network. Self-attention ensures each of these channels contains important information and offers the network a comprehensive understanding of the compressed image. Attending to all the features is especially important in the lower dimensions close to the latent space to guarantee the diminished image size still contains the necessary perceptual and semantic details.

Following the sequence of ResNet and attention blocks, we arrive at a loop of blocks to upsample our image features. Similar to the encoder, we encounter multiple ResNet blocks prior to an operation changing the dimensions of our feature embeddings. The distinction between these loops for the encoder and decoder lie in the number of ResNet blocks prior to changing the feature dimensions. For the encoder, we encountered two ResNet blocks prior to a downsampling operation; the decoder contains three ResNet blocks prior to upsampling. Preserving both the learned semantic and perceptual information throughout the myriad dimension changes of the autoencoder while avoiding the introduction of new errors leads to the requirement for additional convolutional operations. Scaling image features to higher dimensions requires attentive detail to the proximal characteristics of the image. This might seem arbitrary, but shifting image characteristics on the scale of pixels can have a profound effect on image comprehension. Affording an extra block of convolution and stabilization for decoding has been consistently successful and is employed for LDMs as well. Rescaling to satisfy our initial image dimensions requires three passes through this loop, equivalent to the number of passes performed while downsampling our image. Upsampling is performed through a simple nearest-neighbor approach where every element of the features is doubled before its own convolutional operation to ensure the arbitrary doubling of features still satisfies the visual image ratios. Reword?

Exiting the loop of ResNet and upsampling blocks, we encounter three additional ResNet blocks stabilizing the higher-dimensional image feature representations. The 3x3 window of convolutional operations again serve to emphasize the adjacent relationships of image features. Image features have now reached the dimensions of pixel-space and stabilizing their values immediately prior to image conversion becomes very important. Affirming the need for pixel-space stabilization, we pass the image features through group normalization, an activation function, and an outward convolution to convert our high-dimensional image features to a legitimate pixel-space image representation. We have now successfully passed an image through our autoencoder, stabilized our latent space, and are ready to measure the success of our encoding and decoding neural network.

Repeat decoder function and its importance. Decoder is important because at inference time, we are only employing it. Encoder is not utilized whatsoever at inference time.

#### Metric

Two metrics are used to determine the autoencoder's success: perceptual loss and patch-based adversarial loss. 

<p align="center" width="100%">
  <img src="/Stable Diffusion/Images/SD_Images/vgg16_architecture.png" alt="Illustration of a VGG16 convolutional neural network" width="55%"
</p>
  
Perceptual loss measures the semantic understanding of the reconstructed image in comparison to the original. Both the original and reconstructed images are passed through a pre-trained [VGG16](https://www.mygreatlearning.com/blog/introduction-to-vgg16/) convolutional neural network. There are 5 max pooling locations in the VGG16 network (highlighted in red in the image above). The original and reconstructed images' encodings are compared at each of these locations via mean-squared error (MSE). The total MSE is summed across the five output locations to determine the perceptual loss of the reconstructed image. MSE is preferred to typical Euclidean-space losses, such as L2, which depend on pixel-wise comparison. Minimizing the Euclidean distance between two images assumes pixel-wise independence and averages all plausible outputs of the reconstructed image, [encouraging blurring](https://arxiv.org/pdf/1801.03924.pdf). The success of the perceptual loss metric can be [dependent on the network](https://arxiv.org/pdf/2302.04032.pdf) employed for semantic comparisons, and [later literature](https://arxiv.org/pdf/2307.01952.pdf) would demonstrate a decreased emphasis on perceptual loss.

The fact that it says its comparing semantic understanding makes me think that we're comparing latents. Would also explain in Aleksa's walkthrough that the code is applying the scaling before comparing via VGG16. Unlikely, VGG16 is expecting inputs of certain dimensions which would be expected to be in-line with actual image dimensions.
Actually compared at multiple ReLU outputs. Assuming final ReLU before max pooling? Read code to determine.

<p align="center" width="100%">
  <img src="/Stable Diffusion/Images/SD_Images/patch_based_adversarial_loss.jpeg" alt="Example of patch-based adversarial loss" width="40%"
</p>

[Patch-based adversarial loss](https://arxiv.org/pdf/1611.07004.pdf) borrows from GAN theory, introducing controlled patches of noise to reconstructed images while training a discriminator to detect the noisy patches. Introducing localized patches [enforces pixel-space realism](https://arxiv.org/pdf/2012.09841.pdf). Aiding a discriminator in the detection of artificial images by introducing scalar patches of noise encourages the decoder to maintain perceptually important high-frequency details while decoding from the latent space. The patch-based loss is not utilized for an initial chunk of training (50k or so steps), allowing the autoencoder to establish robustness in its encoding-decoding paradigm. Immediately training with both the perceptual and adversarial losses would lead to an overly powerful discriminator and a weaker autoencoder.

Mention that entire purpose of autoencoder training is to create smaller perceptuall-equivalent latent space. We're trying to eliminate unnecessary bits containing high frequency, imperceptible details for most of our training. Here is where the model learns to reapply those high-frequency details to make the images look more realistic.

### Scheduler

The autoencoder solves the first stage of our training: perceptual compression. We've now arrived at a perceptually equivalent and computationally cheaper latent space for our second stage: semantic compression. Here, we'll learn the conceptual composition of images to ensure high fidelity for image synthesis. The semantic learning stage centers on three core components: the scheduler, the U-Net, and the conditioner. Throughout the semantic training stage, the autoencoder is frozen to prevent any changes to its weights, and all learning takes place in the latent space.

Schedulers are algorithmic guides to the denoising process implemented through the U-Net architecture. Training revolves around learning the additive noise process to understand the guided reversal of noise in an image. Many scheduling algorithms have been developed over the years, but were thought to be inextricable from the model architecture until a [2022 paper by Song et. al](https://arxiv.org/pdf/2010.02502.pdf) suggested pre-trained models could utilize different schedulers at inference time within the same family of generative models, and [another paper by Karras et. al](https://arxiv.org/pdf/2206.00364.pdf) confirmed that scheduling algorithms could be entirely separated from the denoising architecture. Schedulers learn the schedule of Gaussian noise addition to images to subsequently model the removal of Gaussian noise from images. The important parameters for these algorithms are: a linear or cosine schedule and a vector linked to the timestep of the iterative noise removal process. We can treat these parameters as a purely mathematical function guiding the U-Net's denoising of latents, thanks to the years of literature that have examined these algorithms for image synthesis performance. For more information on schedulers, I recommend reading the page I wrote focusing on [their literature and evolution](https://github.com/ejohansson13/concepts_explained/blob/main/Stable%20Diffusion/Schedulers_ML.md).

Training: telling U-Net how many timesteps of noise to add (# of timesteps is randomly generated), we add that many timesteps of noise at once (we know mean and variance from scheduling algorithm). We then have the U-Net predict how much noise should be removed at each timestep, compare prediction to ground-truth via L2 norm.

### U-Net

<p align="center" width="100%">
  <img src="/Stable Diffusion/Images/SD_Images/unet_architecture.png" alt="Screenshot of U-Net architecture taken from original research paper" width="40%"
</p>

If you're looking for information on the U-Net architecture, check out [my page](https://github.com/ejohansson13/concepts_explained/blob/main/UNet/UNet_ML.md) offering a brief summarization of its application within image segmentation. The U-Net is an encoder-decoder architecture popularized through its performance in computer vision tasks. In LDMs, the U-Net is responsible for the repetitive denoising of the latent. Ostensibly, the denoising architecture for LDMs does not have to be a U-Net, but [the U-Net's inductive bias for spatial data](https://arxiv.org/pdf/2105.05233.pdf) quickly led to it becoming ubiquitous for denoising in diffusion models. The U-Net functions architecturally similar to its image segmentation application, downsampling its input and expanding the number of channels to determine the conceptually critical image features before upsampling and condensing the number of channels to preserve spatially relevant information.

Within LDMs, U-Nets can be trained to either predict the denoised latent when passed a noisy latent or predict the amount of noise that needs to be removed from a latent to arrive at the denoised latent. Throughout training the U-Net is fed three inputs: the noisy latent, the magnitude of noise that was added to the latent, and the conditioning input. Since the perceptual training stage is complete, we can have confidence in the veracity of our encoded latents. We add a randomly sampled magnitude of noise to a clean, encoded latent to create our noisy latent. This magnitude equates to the timesteps of noise in the forward process we would take to arrive at our randomly sampled noise level. Since the added noise is Gaussian, and we have learned the mean and variance of the forward process from our scheduling algorithm, we can immediately scale up our added noise to our randomly sampled noise level. This noise is added to our clean latent and we pass in both the subsequent noisy latent and the noise level as inputs to the U-Net. More information on the conditioning and its interaction with the U-Net can be found [below](#conditioning). 

Knowing the number of timesteps of noise that was added to our latent, the U-Net begins iterating and progressively denoising the latent. Depending on its mode, either predicting the clean latent or the added noise, the U-Net will output either a latent or the predicted noise added to the latent. In either case, the output is compared to the ground truth via MSE. Throughout training, the U-Net learns how to progressively and iteratively denoise a latent. This becomes crucial at inference time where prompts will be fed in to the U-Net, accompanied by a randomly sampled latent, and an input number of timesteps in which to denoise the latent and synthesize an image. 

### Conditioning

Prompting our diffusion model requires the network's understanding of the prompt. We need an encoder to convert our prompt into embeddings which can interact with our latent embeddings in the U-Net. For the text-to-image case this would be a text encoder but, alternative encoders can deliver prompts through different media while remaining compatible with the LDM architecture. The original LDM paper utilized BERT, but more recent literature has demonstrated that the performance of an image synthesis model is strongly tied to the [performance of its text encoder](https://arxiv.org/pdf/2205.11487.pdf). For this reason, newer diffusion models [have opted for more powerful pre-trained text encoders](https://arxiv.org/pdf/2307.01952.pdf).

The conditioning plays a critical role in guiding the U-Net's denoising of the latent towards a definite destination at inference time. During training, we are afforded the luxury of a pre-determined destination with our provided clean latent. It's important to take advantage of that luxury so the network can correctly train its query, key, and value weight matrices for cross-attention. Latent embeddings serve as the query with textual embeddings providing the roles of key and value vectors. Performing cross-attention between these vectors emphasizes the relationship between the textual prompt and generated image. Cross-attention occurs at every layer for every timestep of the U-Net. Every token in the textual prompt has a thorough impact on the attention the network pays to the latent embeddings throughout the denoising process. By performing cross-attention at every layer, through every downsampling and upsampling operation, we ensure the uninterrupted propagation of information to every stage of the latent's progression through the network. The holistic training approach taken in the semantic training stage allows the U-Net to learn the successful denoising of latents while concurrently learning the conditioning impact on the ultimate latent destination.

Classifier-free guidance.

## Inference

At inference time, the LDM is applying its learned weights through progressive denoising, conditioning, and decoding to align randomly sampled noise with a user-provided natural language text prompt. The end product is expected to be a visually coherent generated image with high-frequency details. Inferring the product begins in the latent space. As mentioned above, Gaussian noise is randomly sampled to form our latent. If the perceptual compression training stage was successful, we will have arrived at an approximately Gaussian distribution in our latent space. This simplifies the sampling process and allows for easy access to a noisy latent expected to follow Gaussian properties. We treat this latent equivalently to noisy latents utilized in the semantic compression training stage. Treating the latent as a meaningfully compressed image with added noise, we begin the denoising process.

### Scheduler

The noisy latent is iteratively fed through the U-Net in conjunction with the provided prompt, progressively removing chunks of noise at every step. Dependent on the sampler schedule, this process may be deterministic or stochastic. From the training process, the U-Net is expected to successfully remove predictable amounts of noise in accordance to the provided timestep. During inference, we provide the U-Net with sequential chunks of timesteps such that the U-Net removes noise proportional to those chunks during each iteration. Given a randomly sampled noisy latent, the sequential denoising process could theoretically lead to any non-deterministic output. By providing conditioning during the semantic training process, the U-Net learns both the successful denoising of a latent and the navigation to a destination latent provided from a conditioning input. The navigation through latent space provided by our conditioning input is 

Inference time schedulers are determined by pre-defined algorithms through previous literature. They are just initialized and applied to latent denoising to determine the amount of noise to be removed.

### U-Net

Pass in two of everything to U-Net for classifier-free guidance. Concatenated x (latent (sampled noise)) with itself, t (number of timesteps to remove noise) with itself, and conditioning c (empty prompt concatenated on to actual text prompt).

### Conditioning (Prompt)

Talk about the interpolation between the provided prompt and the guided denoising of the latent to ultimately result in a newly synthesized image.

Empty conditioning - classifier free guidance, passed through CLIP, used to condition U-Net. Classifier-free diffusion guidance greatly improves sample quality. 

### Decoder

Role in decoding destination latent without introducing any errors. Also mention that due to the patch-based adversarial loss learning, it should be responsible for adding further high-frequency details to our image (polishing image up).




# Citations

VGG16 architecture image - https://pub.towardsai.net/the-architecture-and-implementation-of-vgg-16-b050e5a5920b

Patch-based adversarial loss training image - Rao, S., Stutz, D., Schiele, B. (2020). Adversarial Training Against Location-Optimized Adversarial Patches. In: Bartoli, A., Fusiello, A. (eds) Computer Vision â€“ ECCV 2020 Workshops. ECCV 2020. Lecture Notes in Computer Science(), vol 12539. Springer, Cham. https://doi.org/10.1007/978-3-030-68238-5_32
