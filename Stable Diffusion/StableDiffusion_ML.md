Stable Diffusion was one of multiple text-to-image generation models to generate widespread public attention on the advancements of artificial intelligence and machine learning. Similar to the rise of the transformer architecture for natural language processing tasks, [Latent Diffusion Models (LDMs)](https://arxiv.org/pdf/2112.10752.pdf) were quickly popularized for conditional image generation. Previous image synthesis architectures included [GANs](https://arxiv.org/pdf/1605.05396.pdf), [RNNs](https://arxiv.org/pdf/1502.04623.pdf), and [autoregressive modeling](https://arxiv.org/pdf/1906.00446.pdf) which [multiple](https://arxiv.org/pdf/2102.12092.pdf) [companies](https://arxiv.org/pdf/2206.10789.pdf) favored. However, the performance of diffusion models in zero-shot applications, as well as their efficiency operating in the latent space, led to LDMs becoming the eminent architecture for a variety of generative tasks. Their compatibility with various inputs accelerated the multimodality of machine learning applications. On this page, you'll find an overview of the Stable Diffusion architecture, its training procedure, and inference steps.

This page will on the text-to-image case, but I urge you to check out the [original paper](https://arxiv.org/pdf/2112.10752.pdf) which described multiple other use cases, including image inpainting, conditioning on semantic maps, and layout-to-image, with image generation conditioned on annotated bounding boxes.
<p align="center" width="100%">
  <img src="/Stable Diffusion/Images/SD_Images/LDM_diagram.png" alt="LDM diagram taken from original research paper" width="100%"
</p>

# Architecture

The latent diffusion network follows an encoder-decoder architecure interacting with a compressed, semantic latent space. Between the encoder and decoder blocks, there are three main stages: the scheduler, the denoising U-Net and the custom conditioner (here the focus will be on the text conditioning case). The encoder is responsible for encoding images to their  latent representation. The U-Net is the vehicle carrying this latent representation through the latent space. The prompt suggests the latent space destination, and the scheduler guides the U-Net through the latent space to the preferred destination. This destination, decided by the U-Net, scheduler, and prompt, is the closest latent space representation to the final image. The destination latent is then decoded and transformed to a pixel-space image adhering to the details described in the provided prompt.

Each of these stages will be covered in more detail below, and I've split the architecture details in two. [Training](#training) covers each stage's role throughout the training process. [Inference](#inference) offers the same details for the inference process, describing how user input is synthesized to a 512x512 pixel image.

## Training

Stable Diffusion models are trained on a wide variety of images pulled from the web. A large dataset of images are pumped in for the network to learn a range of visual themes and configurations. These images are taken apart through a forward process consisting of the time-controlled, sequential addition of Gaussian noise to a training image. Controlling the addition of noise and subsequently instructing the network to remove the added noise teaches the network the probability distribution of removing Gaussian noise to generate a coherent image. The process of generating lucid images from noise is referred to as the reverse diffusion process. Importantly, LDMs were not the first network architecture to operate under the diffusion theory. The idea that models can learn to rebuild images from noise by decomposing training images can be found as early as [2015](https://arxiv.org/pdf/1503.03585.pdf). However, previous architectures operated on the pixel-space of images, rendering them prohibitively expensive for high-resolution image generation and reserving their utility for entities who could accommodate the resource-intensive training process. The distinction of latent diffusion models is autological. Rather than operate on the pixel-space, they first compact each image representation, encoding them to a latent space.
<p align="center" width="100%">
  <img src="/Stable Diffusion/Images/SD_Images/LDM_graph.png" alt="Graph demonstrating perceptual vs semantic compression training tradeoff taken from 2022 paper on latent diffusion models" width="50%">
</p>

The above graph demonstrates that the majority of bits constituting a digital image correspond to high-frequency, imperceptible details. In contrast, relatively few bits comprise the semantic information of the image. Unlike previous diffusion models that tried to [jointly balance perceptual and semantic training loss terms](https://arxiv.org/pdf/2106.05931.pdf), latent diffusion models opt for a two-stage approach. Initially, the autoencoder compresses training images, creating a perceptually equivalent, but computationally cheaper latent space for semantic training. Learning the diffusion process is prioritized in the second stage of training. Semantic training freezes the autoencoder weights and masters the reverse diffusion process, learning to construct images from random noise.

### Autoencoder

<p align="center" width="100%">
  <img src="/Stable Diffusion/Images/SD_Images/autoencoder_architecture.png" width="70%"></img>
</p>
<p align="center">
  <em>Image taken from Lilian Weng blog: https://lilianweng.github.io/posts/2018-08-12-vae/</em>
</p>

Autoencoders are a staple of machine learning architectures. Their purpose is to encode and downsample an input to a compressed representation before decoding and upsampling the input to its original dimensionality with minimal information loss. Autoencoders are the first and last stage of Latent Diffusion Models. Their role is to create a stable, learnable latent space containing the distribution of compressed images from which future images can be generated. This space must contain all relevant semantic image information while obscuring superficial details. The latent space of the autoencoder is perceptually equivalent to the pixel-space of images while offering computationally cheaper calculations due to its lower-dimensionality scope.

The encoder performs this initial compression role, with the decoder acting as its complement. Decoders need to successfully rescale lower-dimensional latents to pixel-space images. Their weights are carefully calibrated to avoid the reintroduction of errors when scaling a latent to a higher-dimensional representation. Jeopardizing the fidelity of the image when transitioning to higher dimensions would render latent-dimension modeling obsolete.

Encoding to a latent space requires decisions on the size of the latent space. The authors experimented with multiple downsampling factors, ultimately determining that downsampling factors of 4 or 8 offered the best performance. Downsampling factors of 1 or 2 were prohibitively expensive, operating near pixel-space, and slowed the training rate. Downsampling by a factor of 16 or more led to excessive information loss and low fidelity. Compression at that scale cannibalized the semantic information of the training data. This page will focus on downsampling by a factor of 8, which will be referred to as LDM-8 for the remainder of the page.

#### ResNet blocks
Before explaining the encoder and decoder aspects of the variational auto-encoder, covering their building block is conceptually relevant. [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385), released in 2015, introduced the concept of a residual network for improved performance in image processing tasks. Conventional wisdom held that network depth was inexorably tied to model performance. In reality, unstable gradients and training accuracy degradation highlighted the flaw in arbitrarily deep models. Adding layers and layers in the hopes of modeling an implicit, underlying mapping is inferior to explicitly fitting a referenced, residual mapping. This was the argument made in the ResNet paper. The authors demonstrated that allowing shortcuts between layers of the neural network allowed the network to determine the relevancy and necessity of each layer. Layers deemed unnecessary to the model's decision-making could be mapped to an identity function, allowing the network's signal to propagate unperturbed.

ResNet blocks are residual blocks. Offering a shortcut connection between the input and the propagated signal, they limit each block's responsibility to incremental, residual signal changes. The utility of these blocks and their shortcut connections allow their application in networks of arbitrary depth. Designating two paths for the signal allows the network to determine the relative significance of each path, and weight the correct decision-making path prior to their aggregation. Improved decision-making is compounded by backpropagation. Backpropagation with residual blocks quickly allows larger gradients to flow to earlier layers in the network via the original signal's architectural shortcut. Convolutional kernel weights are dictated by backpropagation, allowing the network to determine the significance of each kernel's contributions. Weights that are deemed unhelpful or unnecessary can be minimized, encouraging the original input to propagate, creating an identity mapping.

<p align="center" width="100%">
  <img src="/Stable Diffusion/Images/SD_Images/residual_block_types.png" width="60%"
</p>
<p align="center">
  <em>Image from [2]</em>
</p>

These blocks can have multiple compositions. A [follow-up paper in 2016](https://arxiv.org/pdf/1603.05027) by the same authors examined these compositions and their relative performance. They determined the full pre-activation option (e in the above diagram) offered the best performance, supported by two findings. First, identity mapping as the shortcut for the original signal encourages optimization, promoting backpropagation to earlier layers, as described above. This accelerates training in the early stages, as the network quickly adjusts its weights in accordance with feedback. Second, normalization in the pre-activation path mitigates overfitting. Consistent regularization of the propagated signal prevents overt weight adoption of the training data distribution, allowing for successful generalization to novel data. The above ResNet block configuration is utilized in the LDM autoencoder as well, illustrated below. 

<p align="center" width="100%">
  <img src="/Stable Diffusion/Images/SD_Images/ResNet_composition.png" width="80%"
</p>

ResNet blocks' success with arbitrarily deep networks defines their success with autoencoders, where network depth is decided by the desired latent space dimensionality. Their ability to combat overfitting and generalize to new data distributions projects well to image compression, where the data distribution encompasses all pixel-space visual configurations. ResNet blocks extract and preserve significant image features with minimal information loss throughout the autoencoder. They also introduce nonlinearity to the model, responsible for modeling the complex relationships and dependencies of image data.

#### Encoder
<p align="center" width="100%">
  <img src="/Stable Diffusion/Images/SD_Images/LDM_encoder_architecture.png" alt="Illustration of LDM encoder" width="100%"
</p>

Constituting the encoder are a mix of ResNet blocks, downsampling, convolution, attention blocks, normalization, and activation functions. [One Stable Diffusion encoder implementation](https://github.com/CompVis/stable-diffusion/tree/main) is illustrated above. Through every step of the encoding operation, the aim is efficient consolidation of image features. Entering the encoder, the image input is convolved with a 3x3 kernel. This convolution broadens the pixel-space RGB image to 128 channels, where pixel-space values are converted to feature embeddings.

Following convolution, a stage of sequential ResNet blocks preceding a downsampling operation are used to progressively halve the height and width dimensions of the image features. The encoder can repeat the progression of image features through this stage until the feature dimensions satisfy the latent space dimensionality. ResNet blocks serve as feature extractors, determining the important semantic information of the input image. Their propagation of semantically significant image features abstracts imperceptible image details. Removing unnecessary high-frequency details from the image while downsampling ensures that all data of latent space dimensionality is perceptually equivalent to their higher dimension counterparts. Compression of half of the data at each downsampling operation requires the downsampling factor to be a power of 2. For LDM-8, data is downsampled three times before exiting the loop and progressing along the encoding path. 

Outside of the downsampling loop, image features are propagated through three additional ResNet blocks. These blocks continue the data progression, preserving and stabilizing image features throughout training. Following these blocks, the training data arrives at an attention block. Throughout the encoder architecture, feature analysis and understanding has been performed by convolution. Convolution is superb in the detection of local image features. Holistic understanding of the image can require a different approach.  Similar to self-attention in text, applying self-attention to image features enables the understanding of longer interdependencies between features and broadens the network's overall image comprehension. The awareness of image features provided by self-attention is especially relevant at lower dimensions which have a higher risk of information loss. Applying self-attention in the encoder, at the latent space dimensionality, mitigates that risk. After the attention block, another ResNet block is applied for stabilization and propagation of the learned image features. At latent space dimensionality, the three sequential ResNet blocks, attention block, and final ResNet block are focused on preserving and stabilizing the learned image features in their compact representations.

Abstracting away semantically meaningless details and arriving at a compact representation of the data, features are passed through a normalization function (data stabilization) and an activation function (complex data modeling) before arriving at a final convolution operation. This convolution determines the number of relevant channels within the latent space, preparing the data both for regularization and semantic training compatibility.

#### KL-regularization

To stabilize the latent space and prevent any pockets of high variance, a low-penalty KL-regularization scheme is applied. [KL-regularization has empirical success](https://proceedings.neurips.cc/paper/2020/file/8e2c381d4dd04f1c55093f22c59c3a08-Paper.pdf) at unifying diverging distributions. This serves to push the latent space to an approximately Gaussian distribution, smoothing out an otherwise unpredictable and high-variance encoding space. The latent space of LDMs are a destination. All diffusion, generation, and denoising at inference time take place in this latent space; stabilizing this space affords a steadier venue for these operations. Approximating a Gaussian distribution becomes important at inference time. Diffusion theory rests on the removal of Gaussian noise, requiring an approximately Gaussian latent space distribution.

The original paper also considered VQ regularization but, Table 8 [in the original LDM paper](https://arxiv.org/pdf/2112.10752) demonstrated superior empirical performance for KL-regularized LDMs compared to their VQ-regularized counterparts. VQ-regularization was built on the theory of the [Vector Quantised-Variational AutoEncoder](https://arxiv.org/pdf/1711.00937) where the latent space of the autoencoder was considered as a discretized code-book of latent vectors.

Despite regularization of the latent space through a KL-divergence penalty intended to push the latent space to a unit variance Gaussian distribution, there was a factor of difference between the scaled autoencoder latent space and the intended approximate Gaussian distribution. This factor of difference can result in a higher signal-to-noise ratio (SNR), affecting image quality. The higher the SNR, the more attention is allocated to semantic detail by the diffusion model early in the denoising process. This results in low-diversity images, sharing a lot of structural similarity, as seen in [Appendix D.1](https://arxiv.org/pdf/2112.10752#page=20&zoom=100,66,118). To account for this, the authors introduced a rescaling factor determined by the component-wise standard deviation of the latents. Dividing by the average standard deviation across latents pushed the latent space to an approximate Gaussian with unit variance, improving image fidelity and diversity. Applying VQ-regularization resulted in latent spaces with approximately unit variance, so no further rescaling was necessary. 

#### Decoder
<p align="center" width="100%">
  <img src="/Stable Diffusion/Images/SD_Images/LDM_decoder_architecture.png" alt="Illustration of LDM decoder" width="100%"
</p>

The decoder is responsible for reconstructing a pixel-space image from the latent with minimal information loss. Concurrently, it is responsible for reintroducing the high-frequency details abstracted away by the encoder. [One implementation](https://github.com/CompVis/stable-diffusion/blob/main/ldm/modules/diffusionmodules/model.py) is depicted above. The decoder complements the encoder and starts with a similar sequence of operations to the end of the encoder path. Latents are broadened from their minimal dimensionality to image features through an initial convolution. Following the convolution, image features progress through an attention block bookended by ResNet blocks. Repeating their functionality from the encoder path, these blocks offer local and holistic perspectives on the compressed image features, preserving the semantically meaningful image information.

Exiting the top row of operations, image features enter an upsampling loop to rescale image dimensions. Three ResNet blocks in conjunction with the upsampling operation control the height and width of the image features. The additional ResNet block when decoding is necessary due to the heightened responsibility of the decoder at inference time. The encoder is unused during inference, while the decoder is responsible for upsampling a randomly sampled denoised latent to a realistic and coherent image. The encoder's role was to create a perceptually equivalent, computationally cheaper, latent space. The decoder's role is the seamless reintroduction of high-frequency visual details, affirming that latents and their decoded pixel-space counterparts are perceptually and conceptually equivalent. The decoder is designated additional parameters, but their utilization is at the network’s discretion. If unnecessary, these parameters can instead model the identity function with no detriment to network performance. Similar to the encoder, this loop is variable and can be enforced depending on the final expected data dimensionality. For LDM-8, this loop is run three times (identical to the encoder). 

Following the upsampling loop, another three ResNet blocks are encountered, continuing the stabilization and propagation of high-dimensional image features. They are further normalized and passed through an element-wise activation function before being convolved to arrive at a pixel-space image with three channels for RGB. 

The architecture presented in the preceding sections was released in 2022, accompanying the research paper. Since then, Stable Diffusion has been released with a 2.0, 2.1, and XL version. Google released Imagen. OpenAI released Dall-E 3. The architectural specifics might change, but their role hasn't. Their fundamental responsibility is the compression of image information to a computationally cheaper latent space. The framework provided above is one possible avenue to achieving that goal, but not the only one.

#### Metric

Two metrics are used to determine the autoencoder's success: perceptual loss and patch-based adversarial loss. 

<p align="center" width="100%">
  <img src="/Stable Diffusion/Images/SD_Images/vgg16_architecture.png" alt="Illustration of a VGG16 convolutional neural network" width="55%"
</p>
  
Perceptual loss, or [LPIPS](https://arxiv.org/pdf/1801.03924), measures the semantic understanding of the reconstructed image in comparison to the original. Both the original and reconstructed images are passed through a pre-trained [VGG16](https://arxiv.org/pdf/1409.1556) convolutional neural network. Following the completion of a convolutional layer (sequence of blue rectangles in above diagram, immediately prior to max pooling), the original and reconstructed image's outputs are compared. Each convolutional layer is terminated with a ReLU function, and each image's features are compared here, prior to the subsequent max pooling operation, via mean-squared error (MSE). Five of these locations exist in the VGG16 architecture. The MSE across the five output locations determines the perceptual loss of the reconstructed image. LPIPS is preferred to typical Euclidean-space losses, such as L1 or L2 loss, which depend on pixel-wise comparison. Minimizing the Euclidean distance between two images assumes pixel-wise independence and averages local pixel differences between images, encouraging blurring. The success of the perceptual loss metric can be [dependent on the network](https://arxiv.org/pdf/2302.04032.pdf) employed for semantic comparisons, and [later literature](https://arxiv.org/pdf/2307.01952.pdf) demonstrates a decreased emphasis on perceptual loss.

<p align="center" width="100%">
  <img src="/Stable Diffusion/Images/SD_Images/patch_based_adversarial_loss.jpeg" alt="Example of patch-based adversarial loss" width="40%"
</p>

[Patch-based adversarial loss](https://arxiv.org/pdf/1611.07004.pdf) borrows from GAN theory, introducing controlled patches of noise to reconstructed images while training a discriminator to detect the noisy patches. Introducing localized patches [enforces pixel-space realism](https://arxiv.org/pdf/2012.09841.pdf). Aiding a discriminator in the detection of generated images by introducing scalar patches of noise encourages the decoder to maintain perceptually important high-frequency details while decoding from the latent space. Patch-based loss is not utilized for an initial phase of training (50k or so steps), allowing the autoencoder to establish robustness in its encoding-decoding paradigm. Immediately training with both the perceptual and adversarial losses would lead to an overly powerful discriminator and a weaker autoencoder.

Perceptual loss is focused on wholly improving the autoencoder. Parameters for both the encoder and decoder halves are tuned in response to the reconstructed image's success in replicating the original image. Patch-based adversarial loss is far more attentive to optimizing the decoder. Successful abstraction of superficial image details leads to a computationally cheaper latent space and more efficient image generation. It also runs the risk of generating bleak or somber images. Maintaining realism requires the successful reintroduction of high-frequency details at the decoding stage. This is the intuition behind patch-based adversarial loss, and why both loss functions were implemented to train the autoencoder.

### Scheduler

The autoencoder solves the first stage of training: perceptual compression. A perceptually equivalent and computationally cheaper latent space has been achieved for the second training stage: semantic compression. Here, the conceptual composition of images will be learned to ensure high fidelity for image synthesis. The semantic learning stage centers on three core components: the scheduler, the U-Net, and the conditioner. Throughout the semantic training stage, the autoencoder is frozen to prevent any changes to its weights, and all learning takes place in the latent space.

Schedulers are algorithmic guides to the denoising process implemented through the U-Net architecture. Training revolves around learning the additive noise process to understand the guided reversal of noise in an image. Many scheduling algorithms have been developed over the years, but were thought to be inextricable from the model architecture until a [2022 paper by Song et. al](https://arxiv.org/pdf/2010.02502.pdf) suggested pre-trained models could utilize different schedulers at inference time within the same family of generative models and, [another paper by Karras et. al](https://arxiv.org/pdf/2206.00364.pdf) confirmed that scheduling algorithms could be entirely separated from the denoising architecture. Schedulers learn the trajectory of Gaussian noise addition to images, allowing them to subsequently model the removal of Gaussian noise from images. The important parameters for these algorithms are a linear or cosine schedule and a vector linked to the timestep of the iterative noise removal process. These parameters can be understood as mathematical functions guiding the U-Net's denoising of latents. For more information on schedulers, I recommend the page I wrote focusing on [their literature and evolution](https://github.com/ejohansson13/concepts_explained/blob/main/Stable%20Diffusion/Schedulers_ML.md).

#### ResNet blocks in Diffusion
Modifying the ResNet blocks utilized in the autoencoder architecture to be compatible with diffusion is the inclusion of temporal information. Diffusion is measured in timesteps. Additive noise and denoising progress are both measured in timesteps. Timesteps serve as a coefficient, proportionally scaling the quantity of noise to be added or removed from the latent. This information can be linearly transformed to adopt the latent dimensionality and the addition or removal of this quantity is the addition or removal of noise from the latent.

<p align="center" width="100%">
  <img src="/Stable Diffusion/Images/SD_Images/ResNet_diffusion_composition.png" width="80%"
</p>

A diagram illustrating the incorporation of time into ResNet blocks is given above. The only change is the inclusion of the timestep vector, linearly transformed to be compatible with the latent dimensionality, and added to the latent prior to the second normalization function.

#### Attention Blocks
In the above section, the integration of noise into the U-Net, via timesteps, was covered. That can also be considered the merging of scheduler functionality with the U-Net. In this section, the integration of conditioning into the U-Net will be covered. Conditioning encodes user directions into embeddings that can interact with the noised latent in vector space. The interaction occurs due to the U-Net’s attention blocks.

These attention blocks’ noteworthy operations are the application of self-attention and cross-attention. Self-attention operates identically to its behavior in the VAE, attending holistically to the latent features and enhancing the network’s understanding. Cross-attention operates as the intersection of conditioning information and the latent. The latent acts as the query vector, with the conditioning taking the role of key and value vectors. 

<p align="center" width="100%">
  <img src="/Stable Diffusion/Images/SD_Images/unet_attn_composition.png" width="80%"
</p>

The architectural implementation of attention blocks aligned with the overall architecture can be seen above. The green arrows between blocks represent natural signal propagation. Blue arrows denote inputs that are summed prior to entering an operation block, while the purple arrows depict the product of the inputs entering the operation block. For example, the output of the self-attention block is added to the output of the 3x3 convolution block before entering the next layer normalization block. The output of the first Linear operation is multiplied by the output of the sequential activation function before being fed into the second Linear operation. Notice the initial input residual has a shortcut to the final output where it is summed with the last convolutional output. This sum is the overall output of each attention block. 

Despite the tangled illustration, the attention block can be viewed as the decoder portion of a [transformer block](https://arxiv.org/pdf/1706.03762). The latent features are treated as the decoder input and self-attended to before feeding into cross-attention as the query vector. The conditioning acts as the encoder output, taking the role of key and value vectors. After performing cross-attention between the latents and the conditioning, the signal continues to a feed-forward network, in this case based around the [GeGLU](https://arxiv.org/pdf/2002.05202v1). The shortcut between the original input and the final output allow the network to determine the significance of the shortcut and the propagating path.

### U-Net

<p align="center" width="100%">
  <img src="/Stable Diffusion/Images/SD_Images/original_unet_architecture.png" alt="Screenshot of U-Net architecture taken from original research paper" width="40%"
</p>

The U-Net is an encoder-decoder architecture popularized through its performance in computer vision tasks. In LDMs, the U-Net is responsible for repetitive denoising of the latent. Ostensibly, the denoising architecture for LDMs does not have to be a U-Net but, the U-Net’s integration of spatial data via its skip connections lends itself to [an inductive bias](https://arxiv.org/pdf/2105.05233.pdf). The U-Net’s success in preserving and propagating semantic image features while downsampling, prior to conserving the spatial information of said image features while upsampling, explains its utilization in denoising diffusion architectures.

Within LDMs, U-Nets are trained to iteratively denoise a latent. Fed a latent, its conditioning, and the timestep of added noise, the model is judged on its accuracy in estimating the added noise. The added noise is entirely determined by the timestep. Diffusion training begins by randomly sampling a timestep. Assuming 1000 steps of added noise are required to destroy an image to the point of resembling white noise, any step between 1 and 1000 can be sampled. One step of added noise is barely noticeable to the image, while 1000 steps of added noise destroy the image to the point of incoherence. Given the mean and variance of the additive Gaussian noise from the scheduling algorithm, the model uses the randomly sampled timestep as a coefficient to scale the added noise. An example is given below, illustrating the effects of adding 50 and 100 time steps of noise to an image.

<img src="/Stable Diffusion/Images/SD_Images/no_added_noise.png" width="33%" /> <img src="/Stable Diffusion/Images/SD_Images/one_added_noise.png" width="33%" /> <img src="/Stable Diffusion/Images/SD_Images/two_added_noise.png" width="33%" />

Given the timestep (scale of added noise applied), the model’s responsibility is to estimate the added noise, such that subtracting the model’s estimation from the noisy latent results in the original, clean latent. Taking the example of 50 timesteps of added noise from above, the model attempts to estimate the quantity of added noise. The model’s prediction is then compared to the ground-truth noise level through MSE, with any loss backpropagating and updating network parameters accordingly. This training process is then repeated. Continuing the example from above, 100 timesteps of added noise are fed into the U-Net, the U-Net estimates the added noise level, loss is calculated through MSE, and backpropagation occurs, tuning network parameters for improved noise estimation in the future. The expectation with this training is that the U-Net architecture learns that t timesteps correspond to a quantity n of noise. 

The U-Net’s responsibility during inference is the progressive denoising of a randomly sampled latent following a user-provided timeline.Training to remove arbitrary levels of noise allows the U-Net to denoise a fixed number of steps at a time to follow the specified denoising timeline.

Diffusion training begins with the random drawing of a timestep. A [seminal work on sampling algorithms](https://arxiv.org/pdf/2206.00364) proffered these timesteps should be drawn from a log-normal distribution. Selecting timesteps from this distribution mirrors the loss curve of image generation models in noise level prediction. Estimating added noise at low noise levels requires the network to discern irrelevant distinctions between the original and noisy latents. Estimating added noise at high noise levels is similar to designing a 3 nm transistor using a paper cutout, laser, and magnifying glass, tools that are too cumbersome for their intended purpose. Subtracting two large quantities of noise in the hope of recovering the original signal (clean latent) is an inefficient method of training, a point made in a [video lecture](https://www.youtube.com/watch?v=T0Qxzf0eaio) from one of the paper’s authors explaining the workaround. For exceedingly high noise levels, rather than estimate the magnitude of added noise to the clean latent, the model’s responsibility is to predict the original, clean latent. This is implemented via a switch in the diffusion training architecture, allowing the U-Net to estimate either the quantity of added noise or the original, unperturbed latent. Since the release of the Latent Diffusion Models paper, estimating the clean latent has become the default as [methods to accelerate the generation process are discovered](#contemporary-image-generation-literature).

<p align="center" width="100%">
  <img src="/Stable Diffusion/Images/SD_Images/unet_architecture.png" width="100%"
</p>

Similar to the encoder and decoder from the autoencoder, one architectural implementation for the U-Net in LDMs is illustrated above. ResNet blocks are readily apparent but, the absorption of conditioning information is also critical and performed through the attention blocks described in the preceding section. Throughout the encoder and decoder paths, a pairing of ResNet and attention blocks are a fixture for the progression of the latent. Timesteps denoting the magnitude of noise interact with the U-Net via ResNet blocks, while conditioning is absorbed by the model through the attention blocks. Each ResNet or attention block in their pairings assimilates the scheduling or conditioning information respectively, guiding the U-Net's progression of the latent.

In this U-Net implementation, at the lowest dimensions, conservation of feature information is emphasized. Only ResNet blocks and a solitary self-attention block direct the progression of the latent. Other approaches to diffusion [emphasized broader attention blocks](https://arxiv.org/pdf/2301.11093) at the lower U-Net dimensions. The decoder path for the U-Net contains one additional block compared to the encoding path, allocating additional parameters to ensure a smooth upscaling procedure. The encoding path in the U-Net requires assimilation of timestep and conditioning information. The decoding path bears the same responsibilities, in addition to incorporating spatial information transmitted through the skip connections. Additional parameters are allocated to account for the added responsibility. Lastly, convolutional blocks bookending the architecture offer control over the number of channels entering and exiting the latent space in addition to their local windows of feature analysis. Supporting the final convolution are normalization and activation function operations offering signal stabilization and nonlinearity for complex modeling.

### Conditioning

Prompting the diffusion model requires comprehension of the provided prompt. An encoder is necessary to convert user intent to embeddings for interaction with the model’s latent embeddings. For the text-to-image case, this would be a text encoder but, alternative encoders can deliver prompts through different media while remaining compatible with the LDM architecture. The original LDM paper utilized [BERT](https://arxiv.org/pdf/1810.04805), but more recent literature has demonstrated that the performance of an image synthesis model is strongly tied to the [performance of its conditioning encoder](https://arxiv.org/pdf/2205.11487.pdf). For this reason, newer text-to-image diffusion models [have opted for more powerful pre-trained text encoders](https://arxiv.org/pdf/2307.01952.pdf).

Conditioning plays a critical role in guiding the U-Net's denoising of the latent towards a definite destination at inference time. Throughout training, the U-Net is learning both the iterative denoising of arbitrarily-sized noise levels and the guidance provided through specific conditioning. Fine-tuning the query, key, and value weight matrices at training time bind model recognition of semantic features, their conditioning, and the latent space destination. The holistic training approach taken in the semantic training stage allows the U-Net to learn the successful denoising of latents while concurrently learning the impact of conditioning on the ultimate latent destination.

Temperature sampling has long been a significant component of [text generation models](https://arxiv.org/pdf/1707.02633) and continues to define [sampling diversity in LLMs](https://www.anthropic.com/claude). A [2021 paper](https://arxiv.org/pdf/2105.05233) introduced the same concept to image generation. The gradients of an image classifier were employed to functionally guide image generation to predefined labels. In that paper, the downsampling path of a U-Net model with attention pooling at the 8x8 layer performed the image classification task and guided image synthesis. This classifier guidance could be operated post-training and operated similarly to temperature sampling for text generation. Scaling the guidance impact led to greater fidelity in image generation with less diversity. The drawbacks to classifier guidance center on the need to train an additional image classifier. The classifier must be trained on latents with the same noising distribution as the diffusion training set, complicating the pipeline. Additionally, the insertion of a classifier gradient during sampling can be viewed as an adversarial attack against the FID and IS metrics, both of which are classifier-based. GANs are inherently based on adversarial learning and have a history of success with the FID and IS metrics. 

[Classifier-free guidance](https://arxiv.org/pdf/2207.12598) was created as a solution to the drawbacks of classifier guidance. Classifier-free guidance jointly trains a conditional and unconditional image generation model in the same neural network architecture by, with some probability, randomly dropping the corresponding textual prompt during training. The authors found that dropout probabilities of 0.1 and 0.2 performed comparably and outperformed a higher dropout probability of 0.5. At inference time, a linear combination of the unconditional and conditional generated images results in the final image. The weighting of the combination can be tuned by a user-provided scale, offering the same tradeoff between fidelity and diversity as provided by classifier-based guidance. Classifier-free guidance resolves many of the drawbacks of classifier guidance while introducing some of its own. 

No additional image classifier is necessary, and training both models can be accomplished on the same architecture with the same parameters, vastly simplifying the training pipeline. Implementing a dropout probability for image captions during training is a trivial addition to the training infrastructure, while there is no necessity for gradient-based improvement which can be viewed as antagonistic and ultimately saturate the network’s learning. [GLIDE](https://arxiv.org/pdf/2112.10741), sharing some authors with the original classifier guidance paper, observed that human evaluators preferred classifier-free guided images and [hypothesized that explicit guidance behaved adversarially towards the model](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#classifier-free-guidance), rather than optimizing the better matches between image and text captions, like classifier-free guidance. The primary drawback of classifier-free guidance is the repeated network pass for the conditional and unconditional image prompts at inference time.

## Inference

At inference time, the LDM applies its learned weights through conditioning, scheduling, denoising, and decoding to align randomly sampled noise with a user-provided, natural language text prompt. The end product is expected to be a visually coherent image with high-frequency details. Inference, like training, begins with random sampling. A latent is randomly sampled from the latent space. Assuming successful regularization of the latent space, the latent will resemble Gaussian noise. This leads to a seamless application of the user-specified sampling process. The randomly sampled noisy latent is progressively denoised in a similar fashion to the training procedure. The U-Net predicts quantities of noise to remove at every iteration, with the number of iterations determined by the user-defined number of inference steps. The U-Net’s denoising trajectory is guided by the conditioner’s encoding of the user prompt to arrive at the correct latent space destination. The decoder upsamples the clean latent and outputs the newly synthesized image.

### Conditioner

At inference time, the conditioner’s responsibility is to encode the user-provided prompt into textual embeddings for interaction with the latent. After the diffusion training, it’s assumed that the cross-attention query, key, and value weight matrices are fine-tuned for seamless integration of the text embeddings with the noisy latent. The conditioning repeats its responsibility from the training process, influencing the iterative denoising process, and guiding the U-Net to the latent destination closest to the pixel-space image depicting the provided prompt. Classifier-free guidance implicitly navigates the denoising process through the latent space. This requires encoding both an empty prompt and the user-provided prompt, both of which have to be passed through the U-Net. The unconditional model output is linearly joined with the conditional generated image to synthesize the final image output.

### Scheduler

The sampler determines the schedule of timesteps employed in denoising the randomly sampled latent. Thanks to [sampling literature](https://arxiv.org/pdf/2206.00364), the sampler used at inference time is not required to be the same sampler used during training. Diffusion models trained with deterministic schedulers can employ any other deterministic or stochastic sampling process at inference time, provided the correct adjustments are made to the generation pipeline. Deterministic sampling, given the same prompt and number of inference steps, will always generate the same image. Stochastic sampling reintroduces noise during the sampling process, offering new concepts at inference time and allowing different images to be generated with the same prompt.

The sampling algorithm tells the U-Net the latent’s denoising progress. Similar to training, the timestep informs the U-Net the magnitude of noise still present in the image and the quantity of noise to be removed at every step. The scheduler navigates the discrepancy between the user-preferred number of inference steps and the number of training steps the model was trained on, selecting a subset of training steps to be used at inference time. After training to remove arbitrary noise levels throughout training, the U-Net resolves the denoising schedule determined by the scheduler and satisfies the selected number of inference steps.

### U-Net

Every neural network is a mathematical approximation of an underlying data distribution. They rely on nonlinear modeling to approximate complex implicit relationships in data. The U-Net is a vehicle approximating the probability distribution of denoising images. Training improves its success in approximating that distribution. It takes steps along the time-controlled denoising trajectory, determining a final coherent image from initial random noise. The more steps taken, the larger the subset of training steps utilized by the scheduler, and the better the approximation of the solution trajectory. This is the reason allowing more steps at inference time results in a higher quality generated image.

One of the reasons for the success of diffusion models over previous architectures is the inductive bias of the U-Net. Its skip connections allow for continuous, repetitive intimations of the spatial component of the data. Given enough data and compute, [successful architectures converge to represent their training data](https://nonint.com/2023/06/10/the-it-in-ai-models-is-the-dataset/). But, diffusion models have an implicit advantage in representing visual data and converge on their dataset in a cheaper and faster fashion. Autoregressive transformers have an intuitive advantage for generating text. Language is inherently dependent on the preceding idea to understand the next idea. Similarly, image generation requires successful spatial awareness of visual configurations. Because of the U-Net’s architecture, it succeeds at tying together conditioning, timesteps, and visual themes in a latent space perceptually equivalent to pixel-space. 

### Decoder

After the diffusion process takes place in the latent space, the decoder’s responsibility is to decode the latent to pixel-space. After training, the decoder is capable of a successful reconstruction of the latent (thanks to the LPIPS metric) and the reintroduction of high-frequency details boosting image realism (thanks to the patch-based adversarial loss). These two training objectives ensure the successful upsampling and transition of the denoised latent to a high fidelity image.

# Contemporary Image Generation Literature

Let’s visit some more recent literature on image generation, and diffusion models in particular. As mentioned in the training and inference sections of this page, the primary drawback to LDMs were the requisite double pass through the architecture generating both conditional and unconditional images in accordance with classifier-free guidance. In an attempt to mitigate this, and accelerate inference speed by combating the number of iterations required for quality sampling, two approaches were developed. 

[Progressive Distillation for Fast Sampling of Diffusion Models](https://arxiv.org/pdf/2202.00512) was a 2022 paper, released two months after the [Latent Diffusion Models](https://arxiv.org/pdf/2112.10752) paper, focused on diffusion in the pixel space. In an effort to quicken the iterative nature of diffusion-based image generation models, progressive distillation trained student models to learn from a pre-trained teacher model, and iteratively halve the number of steps required to generate an image. A teacher model would be trained to operate in the pixel space, requiring 1000 steps to generate high-quality images. A student model would then learn to iteratively predict one denoising step for every two steps taken by the teacher model to arrive at the final image. This student model would be initialized with the exact same architecture and parameters as the teacher model and, upon convergence, would become the new teacher model. A second student model would be initialized with the same parameters and learn from this new teacher model. To make this compatible with the base 2 progression, the initial model would be trained to generate a high-quality image in 1024 steps, the first student model in 512 steps, the subsequent model in 256 steps, etc. Using this formula, researchers were able to distill models to generate competitive images in as few as 4 steps. This training did require a shift in the training objective from prior diffusion models. Rather than estimating a quantity of noise at every iteration, all models in the progressive distillation pipeline were trained to estimate the clean image. The reason for this shift is the wide range of signal-to-noise ratios available when training with many iterations, which becomes unavailable when training with fewer iterations. As these models were distilled, the signal-to-noise ratios grew increasingly compressed, collapsing the link between noise prediction and clean image generation. Similar to the U-Net in LDMs, increasingly distilled models were attempting to subtract large magnitudes of noise from one another and attempting to arrive at a clean signal. Instead, other training objectives were considered with clean image estimation performing the best empirically. The overall cost of training all student models to arrive at a distilled model capable of successful image generation in 4 steps was comparable, or even less than, the cost of training the original diffusion model.

A [follow-up paper in 2023](https://arxiv.org/pdf/2210.03142), featuring the same authors as the original paper (and other superstar diffusion researchers heavily responsible for advancing the field), extrapolated the concept of teacher-student distillation to latent space. This paper also compensated for the advancement of classifier-free guidance by adopting a two-stage approach to progressive distillation. First, a singular, conditional, image generator was trained to match the amalgamated conditional and unconditional model outputs of a classifier-free guided model. Following the training of this singular image generator, multiple student models (initialized with the same architecture and parameters as their corresponding teachers) were trained to generate latents in half as many steps as their teacher models, arriving at a model capable of generating coherent images in as few as 1 to 4 steps. Pivoting the noise estimating, classifier-free guided model to a latent estimating, singular conditional model required 3k gradient updates. Training each subsequent stage required 2k gradient updates until arriving at the low-step distilled models capable of generating in 1, 2, or 4 steps which required 20k gradient updates, a comprehensively cheaper approach than the training of the initial classifier-free guided diffusion model. Additionally, the removal of an obligatory double pass utilized with classifier-free model halves the time and computation necessary with no perceptual performance dropoff. The final models still maintain the optionality of deterministic or stochastic sampling, preserving high diversity in model outputs despite the compressed generation trajectory.

An alternative approach to distillation, released in 2023, [Consistency Models](https://arxiv.org/pdf/2303.01469) mapped images along the same denoising trajectory to their clean destinations. This was accomplished via two approaches: consistency distillation and consistency training. Consistency distillation, similar to progressive distillation, employed a pre-trained diffusion model. The consistency model learned to minimize the difference in generating images given two latents from the same denoising trajectory. Training on adjacent points of the probability flow, the model learned self-consistency in mapping latents at any point along the probability flow to the target image. A new model was trained with its performance compared to the ground-truth image through the LPIPS metric, favored over the L1 or L2 loss metrics. The consistency distillation approach can be viewed similarly to the DDIM theory of deterministically mapping noise to the original signal. There is no iterative denoising through consistency distillation, the model denoises all noisy latents in one step. To improve image quality, noise can be reintroduced, similar to stochastic denoising schedules.

Consistency distilled models extended pre-trained diffusion models in the hopes of accelerating the iterative denoising approach and removing the dual generation impetus of classifier free guidance. Consistency training took a new approach entirely, denoting a new and independent family of generative models, with a new score function. In their approach to solving the consistency training score function, researchers found that higher-order solvers, like Heun, outperformed first-order solvers, supporting a determination by [Karras et. al](https://arxiv.org/pdf/2206.00364). Despite the distinction of training consistency models as opposed to diffusion models, higher-order solvers offered a lower estimation error of the true solution trajectory. Given the same noisy latent, [EDM models](https://arxiv.org/pdf/2206.00364) and consistency trained models were found to generate images with significant semantic similarities, legitimizing consistency trained models as a new family of generative models and implying similar performance to EDM models. In the authors’ experiments, consistency distilled models persistently outperformed their consistency trained counterparts. Consistency models preserve the compute-quality tradeoff synonymous with image generation models, allowing for iterative generation by reintroducing noise in the generative process prior to the final product. Both consistency model approaches were far more computationally expensive than progressive distillation but offered metric-based performance comparable to progressive distillation in half as many steps. The [Dall-E 3](https://cdn.openai.com/papers/dall-e-3.pdf) paper references consistency distillation of its own latent diffusion decoder, supporting the employment of consistency distillation for state of the art image generation models.



# References
[1] LDM paper - https://arxiv.org/pdf/2112.10752

[2] SDXL paper - https://arxiv.org/pdf/2307.01952

[3] ResNet paper - https://arxiv.org/pdf/1512.03385

[4] Improved ResNets - https://arxiv.org/pdf/1603.05027

[5] VGG16 architecture image - https://pub.towardsai.net/the-architecture-and-implementation-of-vgg-16-b050e5a5920b

[6] Patch-based adversarial loss training image - Rao, S., Stutz, D., Schiele, B. (2020). Adversarial Training Against Location-Optimized Adversarial Patches. In: Bartoli, A., Fusiello, A. (eds) Computer Vision – ECCV 2020 Workshops. ECCV 2020. Lecture Notes in Computer Science(), vol 12539. Springer, Cham. https://doi.org/10.1007/978-3-030-68238-5_32

[7] Lilian Weng blog on LDMs - https://lilianweng.github.io/posts/2021-07-11-diffusion-models/

[8] Aleksa Gordic video on LDMs - https://www.youtube.com/watch?v=f6PtJKdey8E

[9] Umar Jamil video on LDMs - https://www.youtube.com/watch?v=ZBKpAp_6TGI
