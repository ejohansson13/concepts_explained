Let's get something out of the way. If you're looking for an intricate overview of the theroetical equation-solving traits baked into differential equations and forming the backbone of their utility as schedulers for image-generating diffusion models, look elsewhere. In this post, we'll be covering the schedulers, or samplers, that initially select the latent noise representation for our diffusion models. We'll be examining the families of schedulers, their similarities and differences, and their respective optimization tricks for producing satisfactory image quality. We will not be examining the theoretical intricacies of these differential equations beyond what is described in their accompanying research papers. Years and years of mathematical theory enabled for the problem-solving tricks that made them employable in machine learning algorithms. We will only be observing them through the lens of their serviceability as noise schedulers for neural networks.

There are two main families of schedulers utilized in Stable Diffusion. We can consider these to be those derived from the DDIM and Euler patriarchs. Both of these papers were inspired by the [original DDPM paper](cite_here). 
